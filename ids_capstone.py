# -*- coding: utf-8 -*-
"""IDS Capstone.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Rb_DVHca1C0nGcHRr1ly9pl5ZMU7-c_M
"""

N_number = 10429495
#Ryan Ghayour's N number (N10429495) used to seed models below

import pandas as pd
import numpy as np
from scipy import stats
import math
# from statsmodels.stats.weightstats import ttest_ind
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Lasso, Ridge
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LassoCV
from sklearn.linear_model import RidgeCV
from scipy.stats import ttest_ind
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import (
    accuracy_score,
    confusion_matrix,
    roc_auc_score,
    auc,
    classification_report
)
from sklearn.metrics import roc_curve, auc
import seaborn as sns
from scipy.stats import gaussian_kde

"""## Loading Data, Preprocessing, and EDA"""

num_headers=['Average Rating','Average Difficulty','number of ratings','pepper','take again','online','male','female']
df_num = pd.read_csv("rmpCapstoneNum.csv", header=None, names=num_headers)
qual_headers=['major','university','state']
df_qual= pd.read_csv('rmpCapstoneQual.csv', header=None, names=qual_headers)
tags_headers = ["Tough grader",	"Good feedback",	"Respected",	"Lots to read",	"Participation matters",
                "Don’t skip class or you will not pass",	"Lots of homework",	"Inspirational",	"Pop quizzes!",	"Accessible",	"So many papers",
                "Clear grading",	"Hilarious",	"Test heavy",	"Graded by few things",	"Amazing lectures",	"Caring",	"Extra credit",	"Group projects",
                "Lecture heavy"]
df_tags= pd.read_csv('rmpCapstoneTags.csv', header=None, names=tags_headers)

# Only keeping rows for professors that have more than 3 ratings
over3ratings = (df_num["number of ratings"]>=4)
# df_num[df_num["number of ratings"]>=4]
# print(over4ratings)
df_num = df_num[over3ratings]
df_qual = df_qual[over3ratings]
df_tags = df_tags[over3ratings]

display(df_num)

print(df_num.describe())

display(df_qual)

print(df_qual.describe())

# Checking missing values
print(f"\n{df_num.isna().sum()}")
print(f"\n{df_tags.isna().sum()}")
print(f"\n{df_qual.isna().sum()}")

"""Only df_num['take again'] has missing values."""

# Exploring 'take again' variable

print(df_num['take again'].describe())

# Pair plot
sns.pairplot(df_num[['take again', 'Average Rating', 'number of ratings']])
plt.suptitle("Pair Plot: 'Take Again', 'Number of Ratings', 'Average Rating'", y=1.02)
plt.show()

# Box plot of average rating for professors with missing vs no missing 'take again' values
df_num_copy = df_num.copy()
df_num_copy['take_again_missing'] = df_num_copy['take again'].isnull()
plt.figure(figsize=(10, 6))
sns.boxplot(data=df_num_copy, x='take_again_missing', y='Average Rating')
plt.title("Box Plot: 'Average Rating' by 'Take Again' Missingness")
plt.xlabel("'Take Again' Missing?")
plt.ylabel("Average Rating")
plt.xticks([0, 1], ['No', 'Yes'])
plt.grid(True)
plt.show()

"""
*   Positive trend between 'take again' and 'Average Rating'
*   Professors with high ratings and a large number of reviews also have missing values for 'take again'
*   The difference in the median 'Average Rating' for professor with missing 'take again' values and for those without missing values is not very large, so we cannot assume that missing 'take again' values correspond to the event that zero students said they would take the class again
*  Seems to be a relevant variable, so we should keep the feature for our models and drop the missing rows when necessary
"""

# Normalizing
df_tags_norm= df_tags.div(df_num['number of ratings'],axis=0)

"""# Functions for 1-3

# Problem 1
"""

# Prep the data
df_trimmed = df_num[['Average Rating', 'Average Difficulty', 'number of ratings', 'male','female']]
m_data = df_trimmed[(df_trimmed['male'] == 1) & (df_trimmed['female'] == 0)].dropna()
f_data = df_trimmed[(df_trimmed['male'] == 0) & (df_trimmed['female'] == 1)].dropna()

data1 = m_data['Average Rating']
data2 = f_data['Average Rating']

mean1, mean2 = np.mean(data1), np.mean(data2)
var1, var2 = np.var(data1, ddof=1), np.var(data2, ddof=1)
n1, n2 = len(data1), len(data2)

#Problem 1
#Welch
t_stat, p_value = stats.ttest_ind(data1, data2, equal_var=False, alternative='greater')
print("Welch Test:")
print(f"t_stat: {t_stat:.5f}")
print(f"p_value: {p_value}")

# Perform KDE using gaussian_kde
male_kde = gaussian_kde(data1, bw_method='scott')
female_kde = gaussian_kde(data2, bw_method='scott')

# Define the range for the x-axis (ratings between 1 and 5)
x = np.linspace(1, 5, 1000)

# Evaluate the KDE on this range
male_density = male_kde(x)
female_density = female_kde(x)

# Plot the KDEs, ensuring that the density is clipped between 1 and 5
plt.figure(figsize=(8, 6))
plt.plot(x, male_density, label='Male', color='blue', linewidth=2)
plt.plot(x, female_density, label='Female', color='pink', linewidth=2)

# Add labels, title, and legend with larger fonts
plt.xlabel('Rating', fontsize=14)
plt.ylabel('Density', fontsize=14)
plt.title('Ratings by Gender', fontsize=24)
plt.legend(fontsize=20)

# Show the plot
plt.tight_layout()
plt.show()

"""# Problem 2"""

#Problem 2
#Levene Statistic
levene_statistic, levene_pvalue = stats.levene(data1, data2)
print("Levene's Test:")
print(f"Statistic: {levene_statistic}")
print(f"P-value: {levene_pvalue}")

# Create the figure and axis
plt.figure(figsize=(10, 6))

# Create the box plot
box_plot = plt.boxplot([data1, data2], labels=['Male Ratings', 'Female Ratings'],
                        patch_artist=True)

# Customize the plot
plt.title('Box Plot of Male and Female Ratings', fontsize=16)
plt.ylabel('Ratings', fontsize=12)

# Color the boxes
colors = ['lightblue', 'lightpink']
for patch, color in zip(box_plot['boxes'], colors):
    patch.set_facecolor(color)

# Add some statistical annotations
plt.text(0.7, plt.gca().get_ylim()[1],
         f'Male Ratings Variance: {np.var(data1):.4f}',
         verticalalignment='top')
plt.text(1.7, plt.gca().get_ylim()[1],
         f'Female Ratings Variance: {np.var(data2):.4f}',
         verticalalignment='top')

# Show the plot
plt.tight_layout()
plt.show()

"""# Problem 3"""

# Critical t-value for 95% confidence interval
data1 = m_data['Average Rating']
data2 = f_data['Average Rating']

pooled_std = math.sqrt((data1.std()**2 * (len(data1) - 1) + data2.std()**2 * (len(data2) - 1)) / (len(data1) + len(data2) - 2))
SEM = pooled_std / math.sqrt(len(data1) + len(data2))

# Confidence interval
mean_diff = mean1 - mean2
margin_of_error = 1.96 * SEM
margin_of_error = 1.96 * SEM
ci_lower = mean_diff - margin_of_error
ci_upper = mean_diff + margin_of_error

print(f"Mean Difference: {mean_diff:.5f}")
print(f"95% Confidence Interval: ({ci_lower:.5f}, {ci_upper:.5f})")


#######################
# Confidence Interval of the variance
np.random.seed(N_number)

# Bootstrap function to calculate variance difference
def bootstrap_variance_difference(group1, group2, num_iterations=10000):
    variance_differences = []
    for _ in range(num_iterations):
        # Resample with replacement
        resampled_group1 = np.random.choice(group1, size=len(group1), replace=True)
        resampled_group2 = np.random.choice(group2, size=len(group2), replace=True)

        # Calculate the variance of each resampled group
        var_group1 = np.var(resampled_group1)
        var_group2 = np.var(resampled_group2)

        # Calculate the difference in variances
        variance_differences.append(var_group1 - var_group2)

    return np.array(variance_differences)


# Perform bootstrapping to get the variance differences
boot_variance_differences = bootstrap_variance_difference(data1, data2)

# Calculate the 95% confidence interval for the difference in variances
lower_bound = np.percentile(boot_variance_differences, 2.5)
upper_bound = np.percentile(boot_variance_differences, 97.5)

#############Get the p-value of bootstrapping
def calculate_bootstrap_pvalue(target_difference, bootstrap_differences):
    # Two-sided p-value for testing if difference is 0
    return np.mean(bootstrap_differences >= target_difference) * 2

# Calculate p-value for difference of 0
p_value = calculate_bootstrap_pvalue(0, boot_variance_differences)
print("P-value (Difference = 0):", p_value)
#################

print(f"Bootstrap Variance Confidence Interval:")
print(f"95% CI: ({lower_bound:.3f}, {upper_bound:.3f})")

boot_variance_differences = bootstrap_variance_difference(data1, data2)
plt.figure(figsize=(10,6))
plt.hist(boot_variance_differences, bins=50)
plt.title('Bootstrap Variance Differences')
plt.xlabel('Variance Difference')
plt.axvline(np.var(data1, ddof=1) - np.var(data2, ddof=1), color='r', linestyle='dashed')
plt.axvline(lower_bound, color='r', linestyle='dashed')
plt.axvline(upper_bound, color='r', linestyle='dashed')
plt.show()

"""# Problem 4"""

#Problem 4

#Initialize tag column index and name
tag_indices = df_tags.columns


#Sample Creation
m_prof_tags = df_tags.loc[(df_num['male'] == 1) & (df_num['female'] == 0)]
f_prof_tags = df_tags.loc[(df_num['female'] == 1) & (df_num['male'] == 0)]


#Normalize tags per professor

#Number of each tag per professor/ num total reviews per professor
m_prof_tags = m_prof_tags.div(df_num['number of ratings'], axis=0)
m_prof_tags.dropna(inplace = True)
f_prof_tags = f_prof_tags.div(df_num['number of ratings'], axis=0)
f_prof_tags.dropna(inplace = True)

#Run significance test for each tag
pvals = []
for tag in tag_indices:
  #Welch test
  pval = stats.ttest_ind(m_prof_tags[tag], f_prof_tags[tag], equal_var = False).pvalue
  pvals.append(pval)
  #print('Welch test p value:', pval)

print('Least Gendered:', sorted(zip(pvals, tag_indices), reverse=True)[:3])
print('Most Gendered:', sorted(zip(pvals, tag_indices), reverse=False)[:3])

#Plotting least gendered tag
plt.title('Pop Quizzes tag by gender')
plt.hist([m_prof_tags['Pop quizzes!'],f_prof_tags['Pop quizzes!']],
         color=['blue','pink'], density=True,
         weights=[np.ones(len(m_prof_tags['Pop quizzes!']))/len(m_prof_tags['Pop quizzes!']), np.ones(len(f_prof_tags['Pop quizzes!']))/len(f_prof_tags['Pop quizzes!'])],
         label=['Male', 'Female'])
plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y*10:.0f}%'))
plt.ylabel('Percentage')
plt.xlabel('Tags per Review')
plt.legend()

#Plotting most gendered tag
plt.title('Hilarious tag by gender')
plt.hist([m_prof_tags['Hilarious'],f_prof_tags['Hilarious']], density=True,
         weights=[np.ones(len(m_prof_tags['Hilarious']))/len(m_prof_tags['Hilarious']), np.ones(len(f_prof_tags['Hilarious']))/len(f_prof_tags['Hilarious'])],
         color=['blue','pink'], label=['Male', 'Female'])
plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y*10:.0f}%'))
plt.ylabel('Percentage')
plt.xlabel('Tags per Review')
plt.legend()

"""# Problem 5"""

#Problem 5

#Sample Creation
m_prof_diff_avgs = df_num['Average Difficulty'].loc[(df_num['male'] == 1) & (df_num['female'] == 0)]
f_prof_diff_avgs = df_num['Average Difficulty'].loc[(df_num['male'] == 0) & (df_num['female'] == 1)]

#Welch test
print('Welch test p value:', stats.ttest_ind(m_prof_diff_avgs, f_prof_diff_avgs,
                                 equal_var = False, nan_policy = 'omit').pvalue)

#Independent samples t test
#print('Independent samples t test p value: ', stats.ttest_ind(m_prof_diff_avgs,
#                                            f_prof_diff_avgs, equal_var = True,
#                                                   nan_policy = 'omit').pvalue)

# Perform KDE using gaussian_kde
male_kde = gaussian_kde(m_prof_diff_avgs, bw_method='scott')
female_kde = gaussian_kde(f_prof_diff_avgs, bw_method='scott')

# Define the range for the x-axis (ratings between 1 and 5)
x = np.linspace(1, 5, 1000)

# Evaluate the KDE on this range
male_density = male_kde(x)
female_density = female_kde(x)

# Plot the KDEs, ensuring that the density is clipped between 1 and 5
plt.figure(figsize=(8, 6))
plt.plot(x, male_density, label='Male', color='blue', linewidth=2)
plt.plot(x, female_density, label='Female', color='pink', linewidth=2)

# Add labels, title, and legend with larger fonts
plt.xlabel('Difficulty', fontsize=14)
plt.ylabel('Density', fontsize=14)
plt.title('Difficulty by Gender', fontsize=24)
plt.legend(fontsize=20)

# Show the plot
plt.tight_layout()
plt.show()

"""# Problem 6"""

#Problem 6
#cohens_d = (f_prof_diff_avgs.mean() - m_prof_diff_avgs.mean()) / (df_num['Average Difficulty'].loc[((df_num['male'] == 1) & (df_num['female'] == 0)) | ((df_num['male'] == 0) & (df_num['female'] == 1))]).std()

#Get number of male and female professors
m_num = len(m_prof_diff_avgs)
f_num = len(f_prof_diff_avgs)

#Calculate pooled standard deviation
pooled_std = math.sqrt((m_prof_diff_avgs.std()**2 * (m_num - 1) + f_prof_diff_avgs.std()**2 * (f_num - 1)) / (f_num + m_num - 2))
#Calculate effect size
effect_size = m_prof_diff_avgs.mean() - f_prof_diff_avgs.mean()

SEM = pooled_std / math.sqrt(m_num + f_num)

lower_bound = effect_size - 1.96*SEM
upper_bound = effect_size + 1.96*SEM

print("Effect Size:", effect_size)
print("Lower Bound:", lower_bound)
print("Upper Bound: ", upper_bound)

#Plot effect size

plt.errorbar(1, effect_size, yerr=1.96*SEM, fmt='o', capsize=5, label='Effect Size')
plt.axhline(0, color='gray', linestyle='--', linewidth=0.8)  # Reference line at 0
plt.xlabel('Studies')
plt.ylabel('Effect Size')
plt.title('Effect Sizes with 95% Confidence Intervals')
plt.legend()
plt.show()

"""# Problem 7

# Predicting average rating from all numerical predictors
"""

# Normalizing
df_num['online'] = df_num['online'] / df_num['number of ratings']
# Dropping NaN values
df_num_cleaned= df_num.dropna()
len(df_num_cleaned)

"""
##Correlation Matrix"""

num_corr_matrix= df_num_cleaned.corr()
print(num_corr_matrix.columns)
# Creating mask for upper triangle
mask = np.triu(np.ones_like(num_corr_matrix, dtype=bool), k=1)

plt.figure(figsize=(10, 8))
sns.heatmap(num_corr_matrix,
           mask=mask,
           annot=True,
           cmap='coolwarm',
           fmt='.2f')
plt.title("Correlation Matrix")
plt.tight_layout()
plt.show()

"""##Scatterplots of each predictor against target variable"""

fig, axes = plt.subplots(3, 3, figsize=(10,10))

predictors = df_num[['Average Difficulty', 'number of ratings', 'pepper', 'take again', 'online', 'male', 'female']].columns

for i, ax in enumerate(axes.flatten()):
    if i < len(predictors):
        ax.scatter(df_num[predictors[i]], df_num['Average Rating'], color='purple', alpha=0.5)
        ax.set_title(f"Scatterplot of \n{predictors[i]} vs. avg_rating")
        ax.set_xlabel(predictors[i])
        ax.set_ylabel("avg_rating")
    else:
        ax.axis('off')

plt.tight_layout()
plt.show()

"""## Modeling


"""

# Identifying predictors and target variable
X = df_num_cleaned[['Average Difficulty','number of ratings','pepper','take again','online','male','female']]
y = df_num_cleaned['Average Rating']

# Splitting the data into training and testing sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=N_number)

# Standardizing features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""###Ridge Regression Model"""

# Using RidgeCV to find optimal alpha
alphas = np.logspace(-5, 5, 100)  # Range of alpha values
ridge_cv = RidgeCV(alphas=alphas, store_cv_values=True)
ridge_cv.fit(X_train_scaled, y_train)

print(f"Best alpha: {ridge_cv.alpha_}")

# Predicting
y_train_pred_ridge = ridge_cv.predict(X_train_scaled)
y_test_pred_ridge = ridge_cv.predict(X_test_scaled)

# Calculating R^2 and RMSE for Ridge Regression model
test_r2_ridge= r2_score(y_test,y_test_pred_ridge)
test_rmse_ridge= np.sqrt(mean_squared_error(y_test,y_test_pred_ridge))

train_r2_ridge= r2_score(y_train,y_train_pred_ridge)
train_rmse_ridge= np.sqrt(mean_squared_error(y_train,y_train_pred_ridge))

"""##Results

###Performance Metrics
"""

# Ridge R² and RMSE results
print("\nRidge Regression Model:")
print(f"\n\tTest R²: {test_r2_ridge:.8f}")
print(f"\tTest RMSE: {test_rmse_ridge:.8f}")
print(f"\n\tTraining R²: {train_r2_ridge:.8f}")
print(f"\tTraining RMSE: {train_rmse_ridge:.8f}")

# Function to display difference in training vs test metrics
def train_test_difference(train_r2, train_rmse, test_r2, test_rmse):
    print("\nR² Difference (Train - Test):", train_r2 - test_r2)
    print("RMSE Difference (Train - Test):", train_rmse - test_rmse)

train_test_difference(train_r2_ridge, train_rmse_ridge, test_r2_ridge, test_rmse_ridge)

"""###Feature Importance"""

# Ridge Regression Coefficients
ridge_ratings_coef_df = pd.DataFrame({
    'Predictors': X.columns,
    'Coefficient': ridge_cv.coef_
}).sort_values('Coefficient', key=abs, ascending=False)
print(f"\n{ridge_ratings_coef_df}")

# Plot for coefficients (betas)
plt.figure(figsize=(10,6))
sns.barplot(x='Coefficient', y='Predictors',
            data=ridge_ratings_coef_df.head(10), palette='Blues_d')
plt.title('Ridge Regression: Feature Importance')
plt.xlabel('Coefficient')
plt.ylabel('Features')
plt.tight_layout()
plt.show()

"""###Model Performance"""

# Function to plot actual vs predicted values from regression model
def plot_predictions_comparison(y_train, y_train_pred, y_test, y_test_pred, title="Model Performance"):

    plt.figure(figsize=(15,10))

    # Training set subplot
    plt.subplot(1, 2, 1)
    plt.scatter(y_train, y_train_pred, alpha=0.3)

    min_val = min(min(y_train), min(y_train_pred))
    max_val = max(max(y_train), max(y_train_pred))
    # plt.plot([min_val, max_val], [min_val, max_val], 'r--')
    plt.plot([.5, 5.5], [.5, 5.5], 'r--')

    r2_train = r2_score(y_train, y_train_pred)
    plt.text(0.05, 0.95, f'R² = {r2_train:.4f}',
            transform=plt.gca().transAxes,
            bbox=dict(facecolor='white', alpha=0.8))
    plt.xlabel('Actual Values')
    plt.ylabel('Predicted Values')
    plt.title('Training Set')
    plt.grid(True, alpha=0.3)

    # Test set subplot
    plt.subplot(1, 2, 2)
    plt.scatter(y_test, y_test_pred, alpha=0.3)

    min_val = min(min(y_test), min(y_test_pred))
    max_val = max(max(y_test), max(y_test_pred))
    # plt.plot([min_val, max_val], [min_val, max_val], 'r--')
    plt.plot([.5, 5.5], [.5, 5.5], 'r--')

    r2_test = r2_score(y_test, y_test_pred)
    plt.text(0.05, 0.95, f'R² = {r2_test:.4f}',
            transform=plt.gca().transAxes,
            bbox=dict(facecolor='white', alpha=0.8))

    plt.xlabel('Actual Values')
    plt.ylabel('Predicted Values')
    plt.title('Test Set')
    plt.grid(True, alpha=0.3)

    plt.suptitle(title, y=1.05)
    plt.tight_layout()
    plt.show()

plot_predictions_comparison(
    y_train, y_train_pred_ridge,
    y_test, y_test_pred_ridge,
    "Ridge Regression Performance"
)

"""##Lasso Regression"""

# Fitting LassoCV
lasso_cv = LassoCV(cv=5, random_state=42)
lasso_cv.fit(X_train_scaled, y_train)

print(f"Best alpha: {lasso_cv.alpha_:.8}")

# Predicting
y_train_pred_lasso = lasso_cv.predict(X_train_scaled)
y_test_pred_lasso = lasso_cv.predict(X_test_scaled)

# Calculating R^2 and RMSE for Ridge Regression model
test_r2_lasso= r2_score(y_test,y_test_pred_lasso)
test_rmse_lasso= np.sqrt(mean_squared_error(y_test,y_test_pred_lasso))

train_r2_lasso= r2_score(y_train,y_train_pred_lasso)
train_rmse_lasso= np.sqrt(mean_squared_error(y_train,y_train_pred_lasso))

"""###Lasso Results"""

# Lasso R² and RMSE results
print("\nRidge Regression Model:")
print(f"\n\tTest R²: {test_r2_lasso:.8f}")
print(f"\tTest RMSE: {test_rmse_lasso:.8f}")
print(f"\n\tTraining R²: {train_r2_lasso:.8f}")
print(f"\tTraining RMSE: {train_rmse_lasso:.8f}")

# Function to display difference in training vs test metrics
def train_test_difference(train_r2, train_rmse, test_r2, test_rmse):
    print("\nR² Difference (Train - Test):", train_r2 - test_r2)
    print("RMSE Difference (Train - Test):", train_rmse - test_rmse)

train_test_difference(train_r2_lasso, train_rmse_lasso, test_r2_lasso, test_rmse_lasso)

# Lasso Regression Coefficients
lasso_ratings_coef_df = pd.DataFrame({
    'Predictors': X.columns,
    'Coefficient': lasso_cv.coef_
}).sort_values('Coefficient', key=abs, ascending=False)
print(f"\n{lasso_ratings_coef_df}")

# Plot for coefficients (betas)
plt.figure(figsize=(10,6))
sns.barplot(x='Coefficient', y='Predictors',
            data=lasso_ratings_coef_df.head(10), palette='Blues_d')
plt.title('Lasso Regression: Feature Importance')
plt.xlabel('Coefficient')
plt.ylabel('Features')
plt.tight_layout()
plt.show()

"""#Problem 8

#Predicting average ratings from all tags

##Correlation Matrix
"""

tags_corr_matrix_features = pd.concat([df_tags_norm, df_num['Average Rating']], axis=1)
tags_corr_matrix= tags_corr_matrix_features.corr()

# Creating mask for upper triangle
mask = np.triu(np.ones_like(tags_corr_matrix, dtype=bool), k=1)

plt.figure(figsize=(15, 10))
sns.heatmap(tags_corr_matrix,
           mask=mask,
           annot=True,
           cmap='coolwarm',
           fmt='.2f')
plt.title("Correlation Matrix")
plt.tight_layout()
plt.show()

"""##Modeling

"""

# Identifying predictors and target variable
X = df_tags_norm
y = df_num["Average Rating"]

# Splitting the data into training and testing sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=N_number)

# Standardizing features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""###Ridge Regression Model"""

# Using RidgeCV to find optimal alpha
alphas = np.logspace(-5, 5, 100)  # Range of alpha values
ridge_cv = RidgeCV(alphas=alphas, store_cv_values=True)
ridge_cv.fit(X_train_scaled, y_train)

print(f"Best alpha: {ridge_cv.alpha_}")

# Predicting
rating_tags_y_train_pred_ridge = ridge_cv.predict(X_train_scaled)
rating_tags_y_test_pred_ridge = ridge_cv.predict(X_test_scaled)

# Calculating R^2 and RMSE for Ridge Regression model
rating_tags_test_r2_ridge= r2_score(y_test, rating_tags_y_test_pred_ridge)
rating_tags_test_rmse_ridge= np.sqrt(mean_squared_error(y_test, rating_tags_y_test_pred_ridge))

rating_tags_train_r2_ridge= r2_score(y_train, rating_tags_y_train_pred_ridge)
rating_tags_train_rmse_ridge= np.sqrt(mean_squared_error(y_train, rating_tags_y_train_pred_ridge))

"""##Results

###Performance Metrics
"""

# Ridge R² and RMSE results
print("\n\nRidge Regression Model:")
print(f"\n\tTest R²: {rating_tags_test_r2_ridge:.8f}")
print(f"\tTest RMSE: {rating_tags_test_rmse_ridge:.8f}")
print(f"\n\tTraining R²: {rating_tags_train_r2_ridge:.8f}")
print(f"\tTraining RMSE: {rating_tags_train_rmse_ridge:.8f}")

# Train/test difference
print(train_test_difference(rating_tags_train_r2_ridge, rating_tags_train_rmse_ridge,
                      rating_tags_test_r2_ridge, rating_tags_test_rmse_ridge))

"""###Feature Importance"""

# Ridge Regression Coefficients
rating_tags_coefficients_ridge_df = pd.DataFrame({
    'Predictors': X.columns,
    'Coefficient': ridge_cv.coef_
}).sort_values('Coefficient', key=abs, ascending=False)
rating_tags_coefficients_ridge= ridge_cv.coef_
print(f"\n{rating_tags_coefficients_ridge_df}")


# Visualizing Ridge Regression Coefficients
plt.figure(figsize=(10, 6))
sns.barplot(x='Coefficient', y='Predictors',
            data=rating_tags_coefficients_ridge_df.head(10), palette='Blues_d')
plt.title('Ridge Regression Feature Importance')
plt.xlabel('Coefficient')
plt.ylabel('Features')

plt.tight_layout()
plt.show()

"""###Model Performance

"""

# Plotting Actual vs Predicted values
plot_predictions_comparison(
    y_train, rating_tags_y_train_pred_ridge,
    y_test, rating_tags_y_test_pred_ridge,
    "Ridge Regression Performance"
)

"""##Lasso Regression Model"""

# Fitting LassoCV
lasso_cv = LassoCV(cv=5, random_state=42)
lasso_cv.fit(X_train_scaled, y_train)

print(f"Best alpha: {lasso_cv.alpha_:.8}")

# Predicting
rating_tags_y_train_pred_lasso = lasso_cv.predict(X_train_scaled)
rating_tags_y_test_pred_lasso = lasso_cv.predict(X_test_scaled)

# Calculating R^2 and RMSE for Lasso Regression modeL
rating_tags_test_r2_lasso= r2_score(y_test, rating_tags_y_test_pred_lasso)
rating_tags_test_rmse_lasso= np.sqrt(mean_squared_error(y_test, rating_tags_y_test_pred_lasso))

rating_tags_train_r2_lasso= r2_score(y_train, rating_tags_y_train_pred_lasso)
rating_tags_train_rmse_lasso= np.sqrt(mean_squared_error(y_train, rating_tags_y_train_pred_lasso))

"""###Lasso Regression Results"""

# Lasso R² and RMSE results
print("\n\nLasso Regression Model:")
print(f"\n\tTest R²: {rating_tags_test_r2_lasso:.8f}")
print(f"\tTest RMSE: {rating_tags_test_rmse_lasso:.8f}")
print(f"\n\tTraining R²: {rating_tags_train_r2_lasso:.8f}")
print(f"\tTraining RMSE: {rating_tags_train_rmse_lasso:.8f}")

# Train/test difference
print(train_test_difference(rating_tags_train_r2_lasso, rating_tags_train_rmse_lasso,
                      rating_tags_test_r2_lasso, rating_tags_test_rmse_lasso))

# Lasso Regression Coefficients
rating_tags_coefficients_lasso_df = pd.DataFrame({
    'Predictors': X.columns,
    'Coefficient': lasso_cv.coef_
}).sort_values('Coefficient', key=abs, ascending=False)
rating_tags_coefficients_lasso= lasso_cv.coef_
print(f"\n{rating_tags_coefficients_lasso_df}")

# Visualizing Lasso Regression Coefficients
plt.figure(figsize=(10,6))
sns.barplot(x='Coefficient', y='Predictors',
            data=rating_tags_coefficients_lasso_df.head(10), palette='Blues_d')
plt.title('Lasso Regression Feature Importance')
plt.xlabel('Coefficient')
plt.ylabel('Features')

# Plotting Actual vs Predicted values
plot_predictions_comparison(
    y_train, rating_tags_y_train_pred_lasso,
    y_test, rating_tags_y_test_pred_lasso,
    "Lasso Regression Performance"
)

"""#Problem 9

#Predicting average difficulty from all tags

## Correlation Matrix
"""

difficulty_corr_matrix_features= pd.concat([df_tags_norm,df_num['Average Difficulty']],axis=1)
difficulty_corr_matrix= difficulty_corr_matrix_features.corr()

# Creating mask for upper triangle
mask = np.triu(np.ones_like(difficulty_corr_matrix, dtype=bool), k=1)

plt.figure(figsize=(15, 10))
sns.heatmap(difficulty_corr_matrix,
           mask=mask,
           annot=True,
           cmap='coolwarm',
           fmt='.2f')
plt.title("Correlation Matrix")
plt.tight_layout()
plt.show()

"""## Modeling"""

# Identifying predictors and target variable
X = df_tags_norm
y = df_num['Average Difficulty']

# Splitting the data into training and testing sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=N_number)

# Standardizing features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""###Ridge Regression Model"""

# Using RidgeCV to find optimal alpha
alphas = np.logspace(-5, 5, 100)  # Range of alpha values
ridge_cv = RidgeCV(alphas=alphas, store_cv_values=True)
ridge_cv.fit(X_train_scaled, y_train)

print(f"Best alpha: {ridge_cv.alpha_}")

# Predicting
difficulty_tags_y_train_pred_ridge = ridge_cv.predict(X_train_scaled)
difficulty_tags_y_test_pred_ridge = ridge_cv.predict(X_test_scaled)

# Calculating R^2 and RMSE for Ridge Regression model
difficulty_tags_test_r2_ridge= r2_score(y_test, difficulty_tags_y_test_pred_ridge)
difficulty_tags_test_rmse_ridge= np.sqrt(mean_squared_error(y_test, difficulty_tags_y_test_pred_ridge))

difficulty_tags_train_r2_ridge= r2_score(y_train, difficulty_tags_y_train_pred_ridge)
difficulty_tags_train_rmse_ridge= np.sqrt(mean_squared_error(y_train, difficulty_tags_y_train_pred_ridge))

"""##Results

###Performance Metrics
"""

# Ridge R² and RMSE results
print("\n\nRidge Regression Model:")
print(f"\n\tTest R²: {difficulty_tags_test_r2_ridge:.8f}")
print(f"\tTest RMSE: {difficulty_tags_test_rmse_ridge:.8f}")
print(f"\n\tTraining R²: {difficulty_tags_train_r2_ridge:.8f}")
print(f"\tTraining RMSE: {difficulty_tags_train_rmse_ridge:.8f}")

# Train/test difference
print(train_test_difference(difficulty_tags_train_r2_ridge, difficulty_tags_train_rmse_ridge,
                      difficulty_tags_test_r2_ridge, difficulty_tags_test_rmse_ridge))

"""###Feature Importance"""

# Ridge Regression Coefficients
difficulty_tags_coefficients_ridge_df = pd.DataFrame({
    'Predictors': X.columns,
    'Coefficient': ridge_cv.coef_
}).sort_values('Coefficient', key=abs, ascending=False)
difficulty_tags_coefficients_ridge= ridge_cv.coef_
print(f"\n{difficulty_tags_coefficients_ridge_df}")

# Visualizing Ridge Regression Coefficients
plt.figure(figsize=(10, 6))
sns.barplot(x='Coefficient', y='Predictors',
            data=difficulty_tags_coefficients_ridge_df.head(10), palette='Blues_d')
plt.title('Ridge Regression Feature Importance')
plt.xlabel('Coefficient')
plt.ylabel('Features')

plt.tight_layout()
plt.show()

"""###Model Performance"""

# Plotting Actual vs Predicted values

plot_predictions_comparison(
    y_train, difficulty_tags_y_train_pred_ridge,
    y_test, difficulty_tags_y_test_pred_ridge,
    "Ridge Regression Performance"
)

"""###Lasso Regression Model"""

# Fitting LassoCV
lasso_cv = LassoCV(cv=5, random_state=42)
lasso_cv.fit(X_train_scaled, y_train)

print(f"Best alpha: {lasso_cv.alpha_:.8}")

# Predicting
difficulty_tags_y_train_pred_lasso = lasso_cv.predict(X_train_scaled)
difficulty_tags_y_test_pred_lasso = lasso_cv.predict(X_test_scaled)

# Calculating R^2 and RMSE for Lasso Regression modeL
difficulty_tags_test_r2_lasso= r2_score(y_test, difficulty_tags_y_test_pred_lasso)
difficulty_tags_test_rmse_lasso= np.sqrt(mean_squared_error(y_test, difficulty_tags_y_test_pred_lasso))

difficulty_tags_train_r2_lasso= r2_score(y_train, difficulty_tags_y_train_pred_lasso)
difficulty_tags_train_rmse_lasso= np.sqrt(mean_squared_error(y_train, difficulty_tags_y_train_pred_lasso))

"""####Lasso Regression Results"""

# Lasso R² and RMSE results
print("\n\nLasso Regression Model:")
print(f"\n\tTest R²: {difficulty_tags_test_r2_lasso:.8f}")
print(f"\tTest RMSE: {difficulty_tags_test_rmse_lasso:.8f}")
print(f"\n\tTraining R²: {difficulty_tags_train_r2_lasso:.8f}")
print(f"\tTraining RMSE: {difficulty_tags_train_rmse_lasso:.8f}")

# Train/test difference
print(train_test_difference(difficulty_tags_train_r2_lasso, difficulty_tags_train_rmse_lasso,
                      difficulty_tags_test_r2_lasso, difficulty_tags_test_rmse_lasso))

# Lasso Regression Coefficients
difficulty_tags_coefficients_lasso_df = pd.DataFrame({
    'Predictors': X.columns,
    'Coefficient': lasso_cv.coef_
}).sort_values('Coefficient', key=abs, ascending=False)
difficulty_tags_coefficients_lasso= lasso_cv.coef_
print(f"\n{difficulty_tags_coefficients_lasso_df}")

# Visualizing Lasso Regression Coefficients
plt.figure(figsize=(10,6))
sns.barplot(x='Coefficient', y='Predictors',
            data=difficulty_tags_coefficients_lasso_df.head(10), palette='Blues_d')
plt.title('Lasso Regression Feature Importance')
plt.xlabel('Coefficient')
plt.ylabel('Features')

# Plotting Actual vs Predicted values
plot_predictions_comparison(
    y_train, difficulty_tags_y_train_pred_lasso,
    y_test, difficulty_tags_y_test_pred_lasso,
    "Lasso Regression Performance"
)

"""# Problem 10

# Predicting whether a professor receives a “pepper” from all available factors (both tags and numerical)
"""

# Concatenating all columns for tags and numerical predictors
df_concat = pd.concat([df_num, df_tags_norm], axis=1)

# Dropping rows with NaN values
df_concat_cleaned=df_concat.dropna()

# Extracting pepper column from new df
pepper= df_concat_cleaned['pepper']

# Identifying predictors and target variable
X= df_concat_cleaned[['Average Rating','Average Difficulty','number of ratings','take again','online','male','female',"Tough grader",	"Good feedback",	"Respected",	"Lots to read",	"Participation matters",
                "Don’t skip class or you will not pass",	"Lots of homework",	"Inspirational",	"Pop quizzes!",	"Accessible",	"So many papers",
                "Clear grading",	"Hilarious",	"Test heavy",	"Graded by few things",	"Amazing lectures",	"Caring",	"Extra credit",	"Group projects",
                "Lecture heavy"]]
y= pepper

logreg_corr_matrix= df_concat_cleaned.corr()

mask = np.triu(np.ones_like(logreg_corr_matrix, dtype=bool), k=1)

# Display correlation matrix with mask
plt.figure(figsize=(15, 10))
sns.heatmap(logreg_corr_matrix,
           mask=mask,
           annot=True,
           cmap='coolwarm',
           fmt='.2f')
plt.title("Correlation Matrix")
plt.tight_layout()
plt.show()

"""There is some colinearity, so we should consider performing dimensionality reduction"""

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=N_number)

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Create and train the logistic regression model
logreg_model = LogisticRegression()
logreg_model.fit(X_train_scaled, y_train)

# Make predictions
y_pred_logreg = logreg_model.predict(X_test_scaled)

# Probabilities
y_prob_logreg = logreg_model.predict_proba(X_test_scaled)[:, 1]

# Print the model's accuracy score
print(f"\nModel Accuracy: {logreg_model.score(X_test_scaled, y_test):.3f}")

# Create confusion matrix visualization
plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_test, y_pred_logreg)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.show()

# Classification report
print(classification_report(y_test, y_pred_logreg))

# Print odds ratios for interpretability
odds_ratios = pd.DataFrame({
    'Feature': X.columns,
    'Odds Ratio': np.exp(logreg_model.coef_[0]),
    'Coefficient': logreg_model.coef_[0]
})
odds_ratios = odds_ratios.sort_values('Odds Ratio', ascending=False)
print("\nTop 5 Features by Odds Ratio:")
print(odds_ratios.head().to_string())

# Calculating ROC
fpr, tpr, thresholds = roc_curve(y_test, y_prob_logreg)
roc_auc = roc_auc_score(y_test, y_prob_logreg)
print(f"\nAUC of the Logistic Regression model: {roc_auc:.3f}")

# Plotting the ROC Curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')
plt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--', label="Random Classifier")
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Logistic Regression ROC Curve')
plt.legend(loc="lower right")
plt.show()

"""## Dimensionality Reduction"""

from sklearn.decomposition import PCA

# Applying PCA to reduce dimensions
pca = PCA()
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

# Checking explained variance ratio
print("\nExplained Variance Vatio:", pca.explained_variance_ratio_)
print("\nCumulative Explained Variance:", pca.explained_variance_ratio_.cumsum())

# Re-training logistic regression on reduced data
log_reg_pca = LogisticRegression()
log_reg_pca.fit(X_train_pca, y_train)

# Predictions on test set
y_pred_pca = log_reg_pca.predict(X_test_pca)

# Probabilities
y_prob_pca = log_reg_pca.predict_proba(X_test_pca)[:, 1]

# Evaluate model performance
print("\nAccuracy on reduced data:", log_reg_pca.score(X_test_pca, y_test))

# Loading predictions and probabilities into df
results_pca = pd.DataFrame({'Predictions': y_pred_pca, 'Probabilities': y_prob_pca})

print(results_pca)

# Classification report
print(classification_report(y_test, y_pred_pca))

# Print the model's accuracy score
print(f"\nModel Accuracy: {log_reg_pca.score(X_test_pca, y_test):.3f}")

# Generating Confusion Matrix
custom_predictions= (y_prob_pca >= 0.4).astype(int)
conf_matrix = confusion_matrix(y_test, custom_predictions)
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Logistic Regression Confusion Matrix')
plt.show()

# Print odds ratios for interpretability
odds_ratios_pca = pd.DataFrame({
    'Feature': X.columns,
    'Odds Ratio': np.exp(log_reg_pca.coef_[0]),
    'Coefficient': log_reg_pca.coef_[0]
})
odds_ratios_pca = odds_ratios_pca.sort_values('Odds Ratio', ascending=False)

# Calculating ROC
fpr, tpr, thresholds = roc_curve(y_test, y_prob_pca)
roc_auc_pca = roc_auc_score(y_test, y_prob_pca)
print(f"\nAUC of the Logistic Regression model: {roc_auc_pca:.3f}")

# Plotting the ROC Curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (PCA) (AUC = {roc_auc_pca:.3f})')
plt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--', label="Random Classifier")
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Logistic Regression ROC Curve (with PCA)')
plt.legend(loc="lower right")
plt.show()

"""## Checking for class imbalance"""

print((df_num['pepper']==0).sum())
print((df_num['pepper']>=1).sum())

"""There is a class imbalance, so we should maximize the recall

# Extra Credit
"""

#Initialize array of most popular disciplines
top_disciplines = []
for major in (df_qual['major'].unique()):
  #Get number of professors in discipline
  num_professors = len(df_qual[df_qual['major'] == major])
  #If a discipline has greater than 500 professors, add to array of most popular
  if (num_professors > 500):
    top_disciplines.append(major)

#Run anova test on all relevant majors
anova_inputs = []
for i in range(len(top_disciplines)):
  prof_pep_vals = (df_num.loc[(df_qual['major'] == top_disciplines[i])])['pepper']
  prof_pep_vals.dropna(inplace = True)
  anova_inputs.append(prof_pep_vals)

print("ANOVA p value: ", stats.f_oneway(*anova_inputs).pvalue)
print("")
print("Psych vs CS: p = ", stats.ttest_ind((df_num.loc[(df_qual['major'] == 'Psychology')])['pepper'], (df_num.loc[(df_qual['major'] == 'Computer Science')])['pepper'], equal_var=False).pvalue)
print("English vs Education: p = ", stats.ttest_ind((df_num.loc[(df_qual['major'] == 'English')])['pepper'], (df_num.loc[(df_qual['major'] == 'Education')])['pepper'], equal_var=False).pvalue)
print('')

peppers_per_professor = []
for subject in (top_disciplines):
  df_num_subject = df_num.loc[(df_qual['major'] == subject)]
  pepper_average = df_num_subject['pepper'].mean()
  peppers_per_professor.append(pepper_average)

#Bar plot of "hotness" by major
plt.barh(top_disciplines, peppers_per_professor, color="purple")
plt.xlabel('Ratio of Professors with a Pepper')
plt.ylabel('Major')
plt.title('Pepper Ratios by Major')


print("Average peppers per subject for each of the most popular disciplines (ranked):")
for i in range(len(peppers_per_professor)):
  index = peppers_per_professor.index(max(peppers_per_professor))
  print(top_disciplines[index], peppers_per_professor[index])
  top_disciplines.pop(index)
  peppers_per_professor.pop(index)